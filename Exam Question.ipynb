{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Matrix Factorization\n",
    "\n",
    "Maximize the following :\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(W, H) = \\sum_{i=1}^I \\sum_{j=1}^J M(i,j) ( Y(i,j) \\log (\\sigma(W(i) H(j))) + (1 - Y(i,j)) \\log(1 - \\sigma(W(i) H(j))) )\n",
    "$$\n",
    "\n",
    "Observed $I\\times J$ binary matrix with possibly missing entries\n",
    "$Y(i,j) \\in \\{0,1\\}$\n",
    "\n",
    "Mask Matrix\n",
    "$M(i,j) = 1$ if $Y(i,j)$ is observed, $M(i,j) = 0$ if $Y(i,j)$ is not observed\n",
    "\n",
    "\n",
    "Here:\n",
    "\n",
    "$\\sigma(x)$ is the sigmoid function defined as\n",
    "\\begin{eqnarray}\n",
    "\\sigma(x) & = & \\frac{1}{1+e^{-x}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "### Properties of the sigmoid function\n",
    "Note that\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\sigma(x) & = & \\frac{e^x}{(1+e^{-x})e^x} = \\frac{e^x}{1+e^{x}} \\\\\n",
    "1 - \\sigma(x) & = & 1 - \\frac{e^x}{1+e^{x}} = \\frac{1+e^{x} - e^x}{1+e^{x}} = \\frac{1}{1+e^{x}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\sigma'(x) & = & \\frac{e^x(1+e^{x}) - e^{x} e^x}{(1+e^{x})^2} = \\frac{e^x}{1+e^{x}}\\frac{1}{1+e^{x}} = \\sigma(x) (1-\\sigma(x))\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\log \\sigma(x) & = & -\\log(1+e^{-x}) = x - \\log(1+e^{x}) \\\\\n",
    "\\log(1 - \\sigma(x)) & = &  -\\log({1+e^{x}})\n",
    "\\end{eqnarray}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "# Generate a random logistic regression problem\n",
    "\n",
    "def sigmoid(t):\n",
    "    return np.exp(t)/(1+np.exp(t))\n",
    "\n",
    "I = 5\n",
    "J = 10\n",
    "\n",
    "# Random Mask \n",
    "M = np.random.rand(I,J)<0.8\n",
    "\n",
    "# Random Parameters\n",
    "W = np.random.randn(I,1)\n",
    "H = np.random.randn(1,J)\n",
    "\n",
    "Y = np.zeros((I,J))\n",
    "# Generate class labels\n",
    "pi = sigmoid(W*H)\n",
    "\n",
    "for i in range(I):\n",
    "    for j in range(J):\n",
    "        if not M[i,j]:\n",
    "            Y[i,j] = np.nan\n",
    "        else:\n",
    "            Y[i,j] = 1 if pi[i,j] < np.random.rand() else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: \n",
    "Given $Y$ and $M$ only find a good $W$ and $H$ by maximizing the objective $\\mathcal{L}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the gradient (Short Derivation)\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathcal{L}(W,H)}{dH} = (M(i,j) (Y(i,j) -\\sigma(W(i) H(j)))) H()\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathcal{L}(W,H)}{dW} = W^\\top (M \\odot (Y -\\sigma(W H)))\n",
    "$$\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py27)",
   "language": "python",
   "name": "py27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
